{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "devfest_dublin_tensorflow_image_classifier.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iFNBVYHKgoTb"
      },
      "source": [
        "# DevFest Dublin 2019 - Tensorflow and Keras - Part II\n",
        "\n",
        "author: [@laura_uzcategui](https://twitter.com/laura_uzcategui)<br />\n",
        "credits to: [Laurence Moroney](https://twitter.com/lmoroney), borrowed images and functions from his Tensorflow in Practice course.\n",
        "\n",
        "On this workshop you will learn a bit more about Machine Learning in general and you will build a model that will help you grasp the following: \n",
        "\n",
        "The workshop is divided in 2 sections, one for tackling the basics and to get you started in tensorflow and keras and the second part where we will be building a Convolutional Neural Net to build an image classifier\n",
        "\n",
        "**Part I**\n",
        "\n",
        "1. Basics of Tensorflow and Keras\n",
        "2. How to build a model \n",
        "3. What are Optimizer and Loss functions and why you need them?\n",
        "4. You will train a Model and get predictions \n",
        "\n",
        "**Part II**\n",
        "\n",
        "5. Preprocessing your data\n",
        "    1. Obtain your data\n",
        "    2. Load your data using image generator \n",
        "    3. Explore your data\n",
        "6. Image classification and Neural Networks\n",
        "    1. Build a model \n",
        "    2. Configure \n",
        "    3. Train your model\n",
        "    4. Test it :)\n",
        "\n",
        "Bonus points: If we have time, we will go through an explanation of Data Augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gTqECBVsixjU"
      },
      "source": [
        "Now that you have learned how to create Models using Tensorflow and Keras, I thought you might like to learn something new and do a bit of a deep dive into the features that Tensorflow offers in order to make your life easier and to be happier when working with Models.\n",
        "\n",
        "Our previous model was just based on predicting numbers and yeah it was cool, but we want do Cooler stuff, so what about building an Image Classifier? In this case we will use Supervised Learning.\n",
        "\n",
        "Ok, let's get this started by obtaining our data\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "PngmDq-w72D7"
      },
      "outputs": [],
      "source": [
        "# Let's install tensorflow 2.x first :)\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "SKNnDfkq77WZ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1OW3QAG77IsH"
      },
      "source": [
        "# Preprocessing your data "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "P8gDe7tAoGXt"
      },
      "source": [
        "\n",
        "\n",
        "### Obtain your data :)\n",
        "\n",
        "Whenever we are working with Machine Learning, we need something to feed the model with and that something is **data**. Data is your most valuable currency when you are working with ML. \n",
        "\n",
        "In this case, we will build a classifier to determine if the image is a dog or a cat. \n",
        "\n",
        "We will use an small version of cats-vs-dogs dataset from the original Kaggle competition: https://www.kaggle.com/c/dogs-vs-cats\n",
        "\n",
        "Colab is linked to your drive folder, lets check the current path and do a list over that path \n",
        "\n",
        "When you want to execute commands as if you were on the terminal, you prefix the command with ```!```. Useful command for debugging or using linux tools."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "gtRi-oWYq4Sa",
        "outputId": "e95ae4a8-dd14-4f0e-cb52-8b98c99e1f13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        " # What is our current working directory\n",
        " !pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "Gkv3V6QHscx9",
        "outputId": "ebf7d746-a937-4f8e-b23b-2d3898d66a10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cats_and_dogs_filtered.zip  sample_data\n"
          ]
        }
      ],
      "source": [
        "# list the files in the current directory \n",
        "!ls\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_Bzx3ddutonJ"
      },
      "source": [
        "**Activity:** Try to obtain the dataset by using the following information:\n",
        "\n",
        "- dataset link: https://storage.googleapis.com/devfest-ml/cats_and_dogs_filtered.zip\n",
        "- path to store dataset: **/content/cats_and_dogs_filtered.zip**\n",
        "\n",
        "Use the command `wget [DATASET] -O [PATH_TO_STORE_DATASET]`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "_oVc-4dDuzdO"
      },
      "outputs": [],
      "source": [
        "# Obtain your dataset :) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "q5cGRBE3wGmU"
      },
      "source": [
        "As we don't have too much time, I've prepared some functions you can use to extract your data and start working with it straight away."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "u1Fr1YIwviM2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "BASE_PATH = '/content'\n",
        "\n",
        "def extract_dataset(fullpath):\n",
        "  ''' will extract the dataset on the fullpath \n",
        "      to the BASE_PATH defined\n",
        "      Args:\n",
        "        fullpath (str): full path name to the zip file\n",
        "  '''\n",
        "  \n",
        "  zip_ref = zipfile.ZipFile(fullpath, 'r')\n",
        "  zip_ref.extractall(BASE_PATH)\n",
        "  zip_ref.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "w61-O5l9xKOF"
      },
      "outputs": [],
      "source": [
        "# Extract the dataset \n",
        "# replace DATASET_ZIPFILE for the actual zip filename\n",
        "extract_dataset('/content/[DATASET_ZIPFILE]')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "X8GnHQT0x3EW"
      },
      "outputs": [],
      "source": [
        "# Check the content of the folder \n",
        "!ls ...."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "QxmIFBUe0Oax"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "s06NqXVbuyiK"
      },
      "source": [
        "### Load your data using image generator \n",
        "\n",
        "---\n",
        "\n",
        "Now let's talk on your folder structure and why it is important to know it :) \n",
        "\n",
        "Whenever we are working with ML, a really good practice and something you should always aim from the very beginning is, once you have your data, you need to partition it in two: \n",
        "\n",
        "- Training set: data used for training your model \n",
        "- Validation set: will be used after training is complete to test that your model is performing well and making predictions correctly.\n",
        "\n",
        "There are multiple ways to do this, currently ML / Data Science libraries provide you with a set of tools / methods or classes to do it. \n",
        "\n",
        "For example: \n",
        "\n",
        "- Scikit has the following: [Train test split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html), which will split randomly the dataset for you.\n",
        "- Tensorflow + Keras have superpowers and it provides you a tool called [Image Data Generator](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator), which allows you to: \n",
        "\n",
        "Tensorflow + Keras have superpowers and they provide you with a tool called Image Data Generator, which allows you to:\n",
        "  - Load the data from a source ( folder, dataframe ) \n",
        "  - Label the data for you \n",
        "  - Normalize data before feeding it to the model\n",
        "  - Augment data \n",
        "\n",
        " This is a really good tool and I would recommend you to learn how to use it, as you will save a lot of time. \n",
        " \n",
        " Now Let's try to use these superpowers to label the data for us and loaded as tensors. \n",
        "\n",
        "<div align=\"middle\">\n",
        "<img src=\"https://storage.googleapis.com/devfest-ml/superhero.png\" width=\"70\"/>\n",
        "</div>\n",
        "\n",
        "As we want to use the Image Data Generator, the directory structure has to look like this: \n",
        "\n",
        "<div align=\"middle\"><img src=\"https://storage.googleapis.com/devfest-ml/dir_structure.png\" width=\"400\"/></div>\n",
        "  \n",
        "  - dataset directory \n",
        "  - 2 subdirectories for training and validation\n",
        "  - Inside ( training / validation) , a directory for each class. \n",
        "\n",
        "  In this case we have 2 classes, cats and dogs. \n",
        "\n",
        "  Now let's go and create our Data Generators. \n",
        "\n",
        "  Example Code: \n",
        "\n",
        "  ```\n",
        "  datagen = ImageDataGenerator(....) \n",
        "  ```\n",
        "\n",
        "Tip: when working on Machine Learning, your data should be normalized before feeding it into the model. \n",
        "\n",
        "As we are feeding images, each image will be read as a pixel with a range from 0 to 255 ( black to white ) and we need to normalize it to be either 0 or 1 ,as neural nets prefer to work with small values. \n",
        "\n",
        "Hint: use the rescale parameter. \n",
        "\n",
        "**Activity:**:\n",
        "- Go back to work and create to data generators, 1 for training and one for validation.\n",
        "\n",
        "Hint: you might need to do imports\n",
        "\n",
        "- Checkout how to import the [ImageDataGenerator](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator): \n",
        "\n",
        "Hint: `from tf.keras.... import ImageDataGenerator` "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "qePqyT-U694i"
      },
      "outputs": [],
      "source": [
        "# Import ImageDataGenerator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "NDME54glzmUp"
      },
      "outputs": [],
      "source": [
        "# Create a training data generator \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "-XkOuXi_zpsU"
      },
      "outputs": [],
      "source": [
        "# Create a validation data generator \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vfLhA0eO1Pd2"
      },
      "source": [
        "Once we have created our generators, we need now to load the data, and we can do this by using [flow_from_directory](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator#flow_from_directory) method, which will take an input directory and will load it into a tensor we could use. \n",
        "\n",
        "\n",
        "Once we have created our generators, we then need to load the data, and we can do this by using flow_from_directory method, which will take an input directory and will load it into a tensor which we can use.\n",
        "\n",
        "\n",
        "\n",
        "**Activity:** Let's load the data by creating 2 generators for training and validation, you need to provide: \n",
        "\n",
        "- Directory to load the data ( tip: create variables pointing to the dirs) \n",
        "- Dimension of the images (150 x 150 pixels),\n",
        "- Batch size, size of the batches of data, will load the data in batches of the amount specified at the time. \n",
        "- The class mode should be binary, as we are only defining between cats or dogs.\n",
        "\n",
        "Example: \n",
        "\n",
        "```\n",
        " datagenerator.flow_from_directory(....) \n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "DgRY7Fph844l"
      },
      "outputs": [],
      "source": [
        "# Define the training and validation directories\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "oUtvX5f72udh"
      },
      "outputs": [],
      "source": [
        "# Create the training generator \n",
        "# Remember to call it with the data generator you created above and the right directory :) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "t1LOViyJ28np"
      },
      "outputs": [],
      "source": [
        "# Create the validation generator \n",
        "# Remember to call it with the data generator you created above and the right directory :) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SnloWgJB0LpW"
      },
      "source": [
        "## Explore your data \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Let's see how our data looks like :) \n",
        "\n",
        "I have created a function for you (borrowed from a course content) , you just need to add few lines of code and we will see a grid of images. \n",
        "\n",
        "If you would like to learn more about how to plot images with matplot lib take a look to : [Subplots](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.subplots.html) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "T5EspSPxpp0N"
      },
      "outputs": [],
      "source": [
        "# we need to import matplotlib :)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plotImages(array_of_images):\n",
        "    # We define a plot of 2 rows and 4 columns of images\n",
        "    fig, axes = plt.subplots(2, 4, figsize=(10,10))\n",
        "    # iterate over each image and (row,col)\n",
        "    for img, ax in zip( array_of_images, axes.flat):\n",
        "        # show the image on the right axis\n",
        "        ax.imshow(img)\n",
        "        ax.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "R56DrqoQLTJw"
      },
      "source": [
        "Now we need to use the function along with the generator we created above for training to get a batch of images we can use :)\n",
        "\n",
        "But first we need to extract a batch of images and you can do it by using next on the training generator\n",
        "```\n",
        "next(train_data_gen)\n",
        "```\n",
        "\n",
        "This will return a tuple with an array with a batch of images and labels. For now we are interested in keeping the images, so when declaring the array, be sure to ignore the labels.\n",
        "\n",
        "**Activity**: \n",
        "- Declare a variable to receive your images \n",
        "- Call the function above indexing the array like this `array_name[:8]` as you only want the first 8 :) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "YNo0wueFjg9H"
      },
      "outputs": [],
      "source": [
        "# Declare an object (array) to receive your images\n",
        "\n",
        "# Call the plotImages with the object you just declared \n",
        "\n",
        "# Output should be a 2 by 4 grid full of images\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bmB2rqx9je9c"
      },
      "source": [
        "# Image Classification and Neural Networks \n",
        "\n",
        "Let's start by working through a few concepts, then will move on to practice.\n",
        "\n",
        "Do you remember when you were younger and notifications pop up in your social media about you appearing in a photo and then you totally panic because it could be actually your worst photo ever?\n",
        "\n",
        "\n",
        "<div align=\"middle\">\n",
        "<img src=\"https://storage.googleapis.com/devfest-ml/drunk.jpg\" width=180> \n",
        "</div>\n",
        "\n",
        "Well, that actually happened thanks to image classification and computer vision techniques that effectively allow the algorithm ( model ) to detect it was you and put a tag ( label ) on the picture. \n",
        "\n",
        "To go into more detail, the core of image recognition is produced by **Convolutional Neural Networks (CNNs)**. \n",
        "\n",
        "The idea behind CNNs is that it can *Detect patterns* on the image by using Filters, practically speaking it can detect features as the filter is passed over the image. \n",
        "\n",
        "When you do image processing you can use filters ( kernels ) to change the image and perform activities like: \n",
        "\n",
        "- Edge detection \n",
        "- Sharpening \n",
        "- Blurring \n",
        "\n",
        "As you are passing the filter over the image, you condense the image down to the important features. \n",
        "\n",
        "All of these is done thanks to the magic of [Convolutions](https://en.wikipedia.org/wiki/Convolution), which mathematically speaking is a math operation over 2 functions that produces another function that explains how the shape of one of the functions is modified by the other one. \n",
        "\n",
        "Phew that was difficult to explain, but check out the following 2 images :) and you will get a better understanding. \n",
        "\n",
        "<div align=\"middle\">\n",
        "<img src=\"https://storage.googleapis.com/devfest-ml/filter.gif\" width=180> \n",
        "</div>\n",
        "\n",
        "As your filter pass over the image, what it is really doing is producing new images emphasizing the features. \n",
        "\n",
        "<div align=\"middle\">\n",
        "<img src=\"https://storage.googleapis.com/devfest-ml/edge_detection.gif\" width=400> \n",
        "</div>\n",
        "\n",
        "Yet another concept that is really useful to understand is that after passing the filter and getting the features emphasized we might want also to apply Max Pooling, which is a way to compress the image with the pixels that are more relevant on the image. \n",
        "\n",
        "**How does MaxPooling works?**\n",
        "\n",
        "- From a group of pixels from the image\n",
        "- You take the biggest\n",
        "- Keep it :) \n",
        "\n",
        "Checkout the animation below to understand it\n",
        "\n",
        "<div align=\"middle\">\n",
        "<img src=\"https://storage.googleapis.com/devfest-ml/maxpool.gif\" width=400> \n",
        "</div>\n",
        "\n",
        "If you are curious to do a deep dive on how the convolutional neural networks are created, checkout this lecture from Standford:[ http://cs231n.github.io/convolutional-networks/](http://cs231n.github.io/convolutional-networks/)\n",
        "\n",
        "### Ok, that's enough theory, show me this running :) \n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8ZB43tBU8EET"
      },
      "source": [
        "## Build your first Convolutional Neural Net \n",
        "\n",
        "Do you remember the **pancakes** from the first section ? Now it's time to use them again. \n",
        "\n",
        "This time we will create a Sequential model, using 3 different classes for building layers in Tensorflow. \n",
        "\n",
        "- [Conv2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D): will create a layer with convolutions and will produce a tensor output \n",
        "- [MaxPool2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D): it will downsample our tensor, specifying a pool size\n",
        "- [Flatten](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten): will flatten the input into tensor that will be feeding the DNN\n",
        "\n",
        "In the snippet below you can see how a CNN could be created.\n",
        "\n",
        "```\n",
        "model = Sequential([\n",
        "    Conv2D(16, 3, activation='relu', input_shape=(28,28,3)),\n",
        "    MaxPooling2D(),\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "```\n",
        "\n",
        "**Activity**: now let's try to create your own CNN with the following requirements: \n",
        "\n",
        "- 3 Conv2D Layers, the first one needs `input_shape` as parameter, with the size of your image as you specified on the training generator with 3 channels to say it's a color photo , and **relu** activation\n",
        "- 3 Max Pooling Layers \n",
        "- 1 Flatten layer \n",
        "- 2 Dense layers:\n",
        "  - First layer with 512 units, and relu activation\n",
        "  - Final layer with only 1 output unit and using the **sigmoid** activation function\n",
        "\n",
        "Checkout this video about what [Activation Functions](https://www.youtube.com/watch?v=m0pIlLfpXWE) are, specifically there is a good explanation for relu (Rectified Linear Unit) and Sigmoid. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "zX308LILvzvG"
      },
      "outputs": [],
      "source": [
        "# Create your CNN here :) \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oafD7KdC8AI-"
      },
      "source": [
        "One cool way to check how your model looks like is by using [Summary](https://www.tensorflow.org/api_docs/python/tf/keras/Model#summary/) method. \n",
        "\n",
        "The summary method gives you an overview over the layers, how many parameters is on each layer and how many are passing to the next.\n",
        "\n",
        "Try it out below\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "fEHmO_c28eEy"
      },
      "outputs": [],
      "source": [
        "# Summary - uncomment the line below \n",
        "[NAME_OF_YOUR_MODEL].summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BJ2_y9-R8rd2"
      },
      "source": [
        "## Configure your Model \n",
        "\n",
        "**Activity**: let's go and try to configure your model as you did on the previous section with the following parameters:\n",
        "\n",
        "- Use [RMSProp](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/RMSprop) as Optimizer , with a learning rate of : 0.0001\n",
        "- Use a loss function: [Binary cross entropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses/binary_crossentropy), this function will help you to measure the probability of the prediction being of certain class \n",
        "\n",
        "Binary cross entropy loss is a loss function commonly used for binary classification. If you want to better understand how cross entropy works and how it relates to this function, I would encourage you to watch this nice video from [A Short Introduction to Entropy, Cross-Entropy and KL-Divergence](https://www.youtube.com/watch?v=ErfnhcEV1O8) by [Aurelien Geron](https://twitter.com/aureliengeron)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "4obPfS10_IFG"
      },
      "outputs": [],
      "source": [
        "## Configure your model here :) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dSxiFkBO_LYx"
      },
      "source": [
        "## Training your Model - nearly there :-)\n",
        "\n",
        "Now we finally get to the real deal, as we will be training our Model to classify pictures between cats and dogs. \n",
        "\n",
        "As you will be training the model, we need to use [fit_generator](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit_generator) method which will actually use the training generator we have created above on the Data Processing.\n",
        "\n",
        "**fit_generator** will train the model on a batch by batch fashion as you will be using the training generator.\n",
        "\n",
        "This runs in parallel to training the model, this means that meanwhile is doing training on GPU is also doing Data Augmentation on CPU. \n",
        "\n",
        "We won't be doing any data augmentation in the workshop, but If you are curious about it and how it could help to improve your model accuracy, I'll recommend you to watch [this](https://www.youtube.com/watch?v=JI8saFjK84o) explanation from the [Deep Learning specialization by deeplearning.ai](https://www.deeplearning.ai/deep-learning-specialization/)\n",
        "\n",
        "Checkout the fit_generator snippet and try to build your own: \n",
        "\n",
        "```\n",
        "history = model.fit_generator(\n",
        "    train_data_gen,\n",
        "    steps_per_epoch=total_train // batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=val_data_gen,\n",
        "    validation_steps=total_val // batch_size, \n",
        "    verbose=0 or 1 or 2\n",
        ")\n",
        "```\n",
        "\n",
        "Arguments: \n",
        "- **steps_per_epoch and validation_steps**: will be tipically the total amount of samples in your training/validation generator divided by the batch size you defined when working on the generator\n",
        "- **epochs**: the number of iterations you are going to do training.\n",
        "\n",
        "**Activity**: \n",
        "\n",
        "Train your CNN by passing the following arguments: \n",
        "\n",
        "- Your training and validation generator defined previously\n",
        "- Use 15 epochs \n",
        "- Verbose set to 2 ( so we can see the progress ) \n",
        "- Calculate your validation_steps and steps_per_epoch accordingly (dataset_size / batch_size) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "olW99RZQ_Y3l"
      },
      "outputs": [],
      "source": [
        "# Define your training here \n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Hvu9ZMDalMNC"
      },
      "source": [
        "### Congratulations :-) you just have trained your first CNN\n",
        "\n",
        "There is a couple of things to note from training, take a look to the output.\n",
        "\n",
        "* **You will see 4 values ( metrics ) pop out**: \n",
        "\n",
        "  ```\n",
        "  Epoch 1/20\n",
        "  100/100 - 11s - loss: 0.7638 - acc: 0.5780 - val_loss: 0.6354 - val_acc: 0.6760\n",
        "  ``` \n",
        "\n",
        "    - loss and acc, are the loss and accuraccy metrics corresponding to how your model is doing over training. What this really means is that the model will make a guess that will be compared against the real value by using the loss function and accuraccy will be calculating how many predictions were actually correct. \n",
        "\n",
        "    Same applies for val_loss and val_acc, but in this case it will be compared against the validation set , which is data the model hasn't seen previously. \n",
        "\n",
        "- **Did you notice how the validation accuracy is lower than the training accuraccy ?** \n",
        "\n",
        "  The answer to that is that our model is not **Overfitting**, which means the model is not generalizing well with images that it hasn't seen before. \n",
        "  \n",
        "  Perhaps there are cats that are lying down or the ears are different therefore we might want to fix this :) \n",
        "\n",
        "  Usually one way to help avoid overfitting is to do Data Augmentation. \n",
        "\n",
        "  **Bonus Tip:** If you want to improve your model at home, checkout this section from [Data Augmentation - Image Classification Tutorial by Tensorflow](https://www.tensorflow.org/tutorials/images/classification#data_augmentation) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xgd4gZ6I_ZUs"
      },
      "source": [
        "## Running a Prediction \n",
        "\n",
        "Now that you have done all the great work and heavy lifting, it's time have fun and test your CNN by testing with some images trying to make a prediction :) \n",
        "\n",
        "I've set an snippet here (borrowed from a course) that will allow you to upload any picture "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "ok": true,
              "status": 200,
              "status_text": ""
            }
          }
        },
        "colab_type": "code",
        "id": "OIOSeosr_cQy",
        "outputId": "0f7e48a1-1800-4639-cb94-f0c76f4cb24a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-85dfa24d-e038-474d-a9a2-166b64cb045b\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-85dfa24d-e038-474d-a9a2-166b64cb045b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "from google.colab import files\n",
        "from keras.preprocessing import image\n",
        "\n",
        "uploaded=files.upload()\n",
        "keys = uploaded.keys()\n",
        "\n",
        "for fn in keys:\n",
        "\n",
        "  # set the path \n",
        "  path = '/content/' + fn\n",
        "  \n",
        "  # load the image as PIL Format\n",
        "  img=image.load_img(path, target_size=(150, 150))\n",
        "  \n",
        "  # Converts a PIL Image instance to a Numpy array.\n",
        "  img_array=image.img_to_array(img)\n",
        "\n",
        "  # insert a new axis intp the array. \n",
        "  # ref: https://docs.scipy.org/doc/numpy/reference/generated/numpy.expand_dims.html\n",
        "  img_array=np.expand_dims(img_array, axis=0)\n",
        "\n",
        "  # stack the array in sequence vertically\n",
        "  images = np.vstack([img_array])\n",
        "\n",
        "  # predicting images\n",
        "  classes = model.predict(images, batch_size=10)\n",
        "  \n",
        "  print(classes[0])\n",
        "  \n",
        "  # If the prediction is greather than 0 is an Dog \n",
        "  if classes[0]>0:\n",
        "    print(fn + \" is a dog\")\n",
        "  # else this is a cat \n",
        "  else:\n",
        "    print(fn + \" is a cat\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FyLzy0iLby_8"
      },
      "source": [
        "## Celebration \n",
        "\n",
        "Wow !! Congratulations because again you got this far :) \n",
        "\n",
        "<div align=\"middle\">\n",
        "<img src=\"https://storage.googleapis.com/devfest-ml/fireworks.gif\" width=250>\n",
        "</div>\n",
        "\n",
        "You have learnt lots on this workshop, just in case I'll give you a summary on what you just have learned: \n",
        "\n",
        "- Downloaded a dataset\n",
        "- Preprocessed your data by cropping it to (150 by 150 pixels) \n",
        "- Splitted the data in two sets, training and validation (**This step is super important**)\n",
        "- Loaded your data in batches\n",
        "- Created your first Convolutional Neural Network\n",
        "- Trained a binary classifiers\n",
        "- Tested your classifier on data the model has never seen :) \n",
        "\n",
        "I recognize that is a lot of information to take in only 2 hours, if you want to improve your model, I'll encourage you to explore how to do Data Augmentation to avoid overfitting. Let us know if you get more than 90% accuraccy :-) \n",
        "\n",
        "If you want to learn more about Machine Learning, this are really good resources to get you started: \n",
        "\n",
        "- [Machine Learning by Andrew Ng](https://www.coursera.org/learn/machine-learning)\n",
        "- [Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems Book](https://www.amazon.co.uk/dp/1492032646/ref=cm_sw_r_tw_dp_U_x_ilRZDbKRF8SV3)\n",
        "- [Tensorflow in Practice Specialization by Laurence Moroney](https://www.coursera.org/specializations/tensorflow-in-practice)\n"
      ]
    }
  ]
}